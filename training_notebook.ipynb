{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jahid/anaconda3/envs/pytorch1.13/lib/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    #--------------------------------\n",
    "    #  Transformer Encoder parameters\n",
    "    #--------------------------------\n",
    "\n",
    "    encoder_model_name = \"bert-base-cased\"  # change the model name if you want to use a different encoder\n",
    "\n",
    "    max_seq_length = 64  # tokenizer max lenght\n",
    "    train_batch_size = 64\n",
    "    val_batch_size = 64\n",
    "    input_size = 768\n",
    "    hidden_size = 768\n",
    "    \n",
    "    #--------------------------------\n",
    "    #  GAN-BERT specific parameters\n",
    "    #--------------------------------\n",
    "    # number of hidden layers in the generator, \n",
    "    # each of the size of the output space\n",
    "    generator_noise_size = 100\n",
    "    num_hidden_layers_g = 1; \n",
    "    # number of hidden layers in the discriminator, \n",
    "    # each of the size of the input spclassace\n",
    "    num_hidden_layers_d = 1; \n",
    "    # dropout to be applied to discriminator's input vectors\n",
    "    out_dropout_rate = 0.2\n",
    "\n",
    "    # Replicate labeled data to balance poorly represented datasets, \n",
    "    # e.g., less than 1% of labeled material\n",
    "    apply_balance = True\n",
    "\n",
    "    #  Optimization parameters\n",
    "    learning_rate_discriminator = 5e-5\n",
    "    learning_rate_generator = 5e-5\n",
    "    epsilon = 1e-8\n",
    "    num_train_epochs = 10\n",
    "\n",
    "    # Scheduler\n",
    "    apply_scheduler = False\n",
    "    warmup_proportion = 0.1\n",
    "\n",
    "\n",
    "    labeled_file = \"./data/labeled.tsv\"\n",
    "    unlabeled_file = \"./data/unlabeled.tsv\"\n",
    "    test_filename = \"./data/test.tsv\"\n",
    "\n",
    "    label_list = [\"UNK_UNK\",\"ABBR_abb\", \"ABBR_exp\", \"DESC_def\", \"DESC_desc\", \n",
    "                \"DESC_manner\", \"DESC_reason\", \"ENTY_animal\", \"ENTY_body\", \n",
    "                \"ENTY_color\", \"ENTY_cremat\", \"ENTY_currency\", \"ENTY_dismed\", \n",
    "                \"ENTY_event\", \"ENTY_food\", \"ENTY_instru\", \"ENTY_lang\", \n",
    "                \"ENTY_letter\", \"ENTY_other\", \"ENTY_plant\", \"ENTY_product\", \n",
    "                \"ENTY_religion\", \"ENTY_sport\", \"ENTY_substance\", \"ENTY_symbol\", \n",
    "                \"ENTY_techmeth\", \"ENTY_termeq\", \"ENTY_veh\", \"ENTY_word\", \"HUM_desc\", \n",
    "                \"HUM_gr\", \"HUM_ind\", \"HUM_title\", \"LOC_city\", \"LOC_country\", \n",
    "                \"LOC_mount\", \"LOC_other\", \"LOC_state\", \"NUM_code\", \"NUM_count\", \n",
    "                \"NUM_date\", \"NUM_dist\", \"NUM_money\", \"NUM_ord\", \"NUM_other\", \n",
    "                \"NUM_perc\", \"NUM_period\", \"NUM_speed\", \"NUM_temp\", \"NUM_volsize\", \n",
    "                \"NUM_weight\"]\n",
    "\n",
    "    label2class = {cls: idx for idx, cls in enumerate(label_list)}\n",
    "\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import Trainer\n",
    "from modeling import Discriminator, Generator\n",
    "from tensor_dataset import (\n",
    "    process_all_data, get_qc_examples_from_file, get_tensor_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jahid/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/jahid/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/jahid/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/jahid/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/jahid/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.encoder_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(\"How many pitchers occupy the shelf beside the crouching woman in Edgar Degas 's 1886 painting The Tub ?\",\n",
       "   'NUM_count'),\n",
       "  ('Which side of the face do most artists tend to show more of in self-portraits ?',\n",
       "   'ENTY_other')],\n",
       " [(\"What film ends with the line : `` This is Mrs. Norman Maine '' ?\",\n",
       "   'UNK_UNK'),\n",
       "  ('What is the average salary of a paleontologist ?', 'UNK_UNK')],\n",
       " [('How far is it from Denver to Aspen ?', 'NUM_dist'),\n",
       "  ('\"What county is Modesto ', 'LOC_city')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the examples\n",
    "labeled_examples = get_qc_examples_from_file(cfg.labeled_file)\n",
    "unlabeled_examples = get_qc_examples_from_file(cfg.unlabeled_file)\n",
    "test_examples = get_qc_examples_from_file(cfg.test_filename)\n",
    "\n",
    "labeled_examples[:2],unlabeled_examples[:2], test_examples[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data: 5452, Labeled Data: 109, Masking Ratio: 0.01999266324284666\n"
     ]
    }
   ],
   "source": [
    "train_examples = labeled_examples\n",
    "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
    "\n",
    "if unlabeled_examples:\n",
    "  train_examples = train_examples + unlabeled_examples\n",
    "  tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
    "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
    "\n",
    "train_data = process_all_data(train_examples, train_label_masks, balance_label_examples=True)\n",
    "\n",
    "train_dataloader = get_tensor_dataset(\n",
    "  train_data,\n",
    "  label_mapper=cfg.label2class,\n",
    "  tokenizer=tokenizer,\n",
    "  max_seq_length=cfg.max_seq_length,\n",
    "  batch_size=cfg.train_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
    "test_data = process_all_data(test_examples, test_label_masks, balance_label_examples=False)\n",
    "\n",
    "test_dataloader = get_tensor_dataset(\n",
    "  test_data,\n",
    "  label_mapper=cfg.label2class,\n",
    "  tokenizer=tokenizer,\n",
    "  max_seq_length=cfg.max_seq_length,\n",
    "  batch_size=cfg.train_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jahid/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/jahid/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "encoder = AutoModel.from_pretrained(cfg.encoder_model_name)\n",
    "\n",
    "discriminator=Discriminator(\n",
    "    input_size=cfg.input_size, \n",
    "    hidden_sizes=[cfg.hidden_size],\n",
    "    num_labels=len(cfg.label_list)+1,\n",
    "    dropout_rate=cfg.out_dropout_rate\n",
    ")\n",
    "\n",
    "generator = Generator(\n",
    "    noise_size=cfg.generator_noise_size,\n",
    "    output_size=cfg.hidden_size,\n",
    "    hidden_sizes=[cfg.hidden_size],\n",
    "    dropout_rate=cfg.out_dropout_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Generator(\n",
       "   (layers): Sequential(\n",
       "     (0): Linear(in_features=100, out_features=768, bias=True)\n",
       "     (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "     (2): Dropout(p=0.2, inplace=False)\n",
       "     (3): Linear(in_features=768, out_features=768, bias=True)\n",
       "   )\n",
       " ),\n",
       " Discriminator(\n",
       "   (input_dropout): Dropout(p=0.2, inplace=False)\n",
       "   (layers): Sequential(\n",
       "     (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "     (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "     (2): Dropout(p=0.2, inplace=False)\n",
       "   )\n",
       "   (logit): Linear(in_features=768, out_features=52, bias=True)\n",
       "   (softmax): Softmax(dim=-1)\n",
       " ))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer =  Trainer(\n",
    "    config=cfg,\n",
    "    encoder=encoder,\n",
    "    discriminator=discriminator,\n",
    "    generator=generator,\n",
    "    train_loader=train_dataloader,\n",
    "    val_loader=test_dataloader,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "trainer.to_device()\n",
    "trainer.configure_optimizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [01:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metric:  {'generator_loss': 0.5601518516955168, 'discriminator_loss': 4.763273303923399}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:01,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metric:  {'validation_accuracy': 0.364, 'validation_loss': 2.9671733379364014}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [01:03<25:14, 63.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [01:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metric:  {'generator_loss': 0.770343665195548, 'discriminator_loss': 2.768962702025538}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metric:  {'validation_accuracy': 0.492, 'validation_loss': 2.3955118656158447}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [02:05<24:03, 62.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [01:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metric:  {'generator_loss': 0.7469146186890809, 'discriminator_loss': 1.9927453554194907}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:01,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metric:  {'validation_accuracy': 0.54, 'validation_loss': 2.2363500595092773}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [03:08<23:00, 62.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [01:01,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metric:  {'generator_loss': 0.7338307759036189, 'discriminator_loss': 1.5411683081284813}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:01,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metric:  {'validation_accuracy': 0.584, 'validation_loss': 2.022867441177368}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [04:11<22:02, 62.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [01:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metric:  {'generator_loss': 0.7309363568606584, 'discriminator_loss': 1.2716243688179099}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:01,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metric:  {'validation_accuracy': 0.594, 'validation_loss': 2.005647659301758}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [05:14<21:00, 63.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [01:01,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metric:  {'generator_loss': 0.7241552640562472, 'discriminator_loss': 1.0425708235605904}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:01,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metric:  {'validation_accuracy': 0.628, 'validation_loss': 1.9262101650238037}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [06:18<19:59, 63.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [01:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metric:  {'generator_loss': 0.7212932517995005, 'discriminator_loss': 0.9118098396322002}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:01,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metric:  {'validation_accuracy': 0.618, 'validation_loss': 2.0432677268981934}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [07:20<18:54, 63.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [01:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Metric:  {'generator_loss': 0.7190649768580561, 'discriminator_loss': 0.857661516122196}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:01,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Metric:  {'validation_accuracy': 0.64, 'validation_loss': 2.0537848472595215}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [08:23<17:48, 62.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "trainer.train(epochs=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov  4 2022, 13:48:29) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f51211c33b31b840051e5f0298ba5b245a634bdcb93ef00618270d3e0747448f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
